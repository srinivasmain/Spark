ChatGPT 3.5

User
import unittest
from unittest.mock import patch, MagicMock
from datetime import datetime
from pyspark.sql import SparkSession
from your_module import harvest_dq4qd

class TestHarvestDQ4QD(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        # Initialize Spark session for testing
        cls.spark = SparkSession.builder.master("local[2]").appName("test").getOrCreate()

    @classmethod
    def tearDownClass(cls):
        # Stop Spark session
        cls.spark.stop()

    def setUp(self):
        # Set up test data or mocks if needed
        pass

    def tearDown(self):
        # Clean up after each test if needed
        pass

    def test_fetch_start_end_time(self):
        # Create a DataFrame with test data
        test_data = [("table1", "db1", "col1", "Pass", datetime(2023, 1, 1)),
                     ("table1", "db1", "col1", "Fail", datetime(2023, 1, 2)),
                     ("table2", "db2", "col2", "Pass", datetime(2023, 1, 3))]
        schema = ["table_name", "database_name", "column_name", "rule_status", "created_ts"]
        df = self.spark.createDataFrame(test_data, schema=schema)

        # Mock the logger
        with patch('your_module.logger') as mock_logger:
            start_time, end_time = harvest_dq4qd.fetch_start_end_time(df, datetime(2023, 1, 2))

            # Assert the expected start and end time based on the test data
            self.assertEqual(start_time, datetime(2023, 1, 2))
            self.assertEqual(end_time, datetime(2023, 1, 3))

            # Assert that the logger.info method was called with the expected message
            mock_logger.info.assert_called_with("Start time: 2023-01-02 00:00:00, End time: 2023-01-03 00:00:00")

    def test_get_aggregated_scores(self):
        # Create a DataFrame with test data
        test_data = [("db1", "table1", "col1", "Pass", 1, 0, 0, 1),
                     ("db1", "table1", "col1", "Fail", 0, 1, 0, 1),
                     ("db2", "table2", "col2", "Pass", 1, 0, 0, 1)]
        schema = ["database_name", "table_name", "column_name", "rule_status", "pass_count", "failed_count",
                  "not_executed_count", "total_rules"]
        df = self.spark.createDataFrame(test_data, schema=schema)

        # Mock the logger
        with patch('your_module.logger') as mock_logger:
            result_df = harvest_dq4qd.get_aggregated_scores(df)

            # Assert the expected aggregated scores based on the test data
            expected_result = [("db1", "table1", "col1", 50.0),
                               ("db2", "table2", "col2", 100.0)]
            expected_schema = ["database_name", "table_name", "column_name", "pass_percentage"]
            expected_df = self.spark.createDataFrame(expected_result, schema=expected_schema)

            self.assertTrue(result_df.subtract(expected_df).isEmpty())
            self.assertTrue(expected_df.subtract(result_df).isEmpty())

            # Assert that the logger.info method was called with the expected message
            mock_logger.assert_called_with("No start time provided. Using min-max times. Start time: (start_time_out), End time: 2023-01-03 00:00:00")

    # Add more test cases for other functions as needed

if __name__ == '__main__':
    unittest.main()




import unittest
from unittest.mock import Mock, patch
from datetime import datetime
from your_module import harvest_dq4qd

class TestHarvestDQ4QD(unittest.TestCase):

    @patch("your_module.sparkUtil")
    @patch("your_module.helper")
    @patch("your_module.load_properties")
    def test_harvest_dq4qd(self, mock_load_properties, mock_helper, mock_sparkUtil):
        # Mocking necessary dependencies
        spark_mock = Mock()
        spark_util_mock = Mock()
        load_properties_mock = Mock()
        helper_mock = Mock()
        mock_sparkUtil.create_db_dataframe.return_value = spark_util_mock
        mock_load_properties.return_value = load_properties_mock
        mock_helper.get_ast_hierarchy_conditions.return_value = ("db_condition", "table_condition", "column_condition")

        # Setting up the test data and environment
        from_time = datetime(2022, 1, 1)
        dq_result_df_mock = spark_util_mock.create_db_dataframe.return_value
        dq_rule_table_df_mock = spark_util_mock.create_db_dataframe.return_value
        dq_agg_table_df_mock = spark_util_mock.create_db_dataframe.return_value
        ast_hierarchy_df_mock = spark_util_mock.create_db_dataframe.return_value

        # Mocking the fetch_start_end_time function
        with patch("your_module.fetch_start_end_time", return_value=(from_time, from_time + timedelta(days=1))):
            # Calling the actual method
            harvest_dq4qd(spark_mock, from_time)

        # Add assertions based on the expected behavior of your method
        mock_sparkUtil.create_db_dataframe.assert_called_with(spark_mock, "dq", "rule result_table")
        mock_sparkUtil.create_db_dataframe.assert_called_with(spark_mock, "dcm", "rule_table")
        mock_sparkUtil.create_db_dataframe.assert_called_with(spark_mock, "dcm", "agg_table")
        mock_sparkUtil.create_db_dataframe.assert_called_with(spark_mock, "dcm", "ast_hierarchy_table")
        mock_sparkUtil.create_db_dataframe.assert_called_once()  # Check if all dataframes are loaded

        mock_sparkUtil.fetch_start_end_time.assert_called_once_with(dq_result_df_mock, from_time)
        dq_result_df_mock.filter.assert_called_once_with((F.col("created_ts") > from_time) & (F.col("created_ts") <= from_time + timedelta(days=1)))
        dq_result_df_mock.dropDuplicates.assert_called_once_with(['table_name', 'database_name', 'column_name', 'rule_def_id'])
        dq_result_df_mock.count.assert_called_once()

        # Continue with more assertions based on your specific requirements and expected behavior

if __name__ == "__main__":
    unittest.main()
