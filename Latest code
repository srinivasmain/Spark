
# Import necessary libraries and modules
from src.com.bofa.dcm.utilities import sparkDBUtility as sparkUtil, dq4qd_logger, constants
from src.com.bofa.dcm.utilities import helperUtils as helper
import pyspark.sql.functions as F
from pyspark.sql.window import Window
from src.com.bofa.dcm.properties import load_properties
import json
import configparser
logger = dq4qd_logger.get_logger_with_name(constants.LOGGER_NAME)

# Function to fetch start and end time based on input conditions
def fetch_start_end_time(dq_result_df, start_time):
    if start_time is not None:
        start_time_out = start_time
        max_created_ts = dq_result_df.agg(F.max("created_ts").alias("end_time"))
        end_time_out = max_created_ts.first()["end_time"]
		logger.info(f"Start time: {start_time_out}, End time: {end_time_out}")
        return start_time_out, end_time_out
    else:
        min_max_df = dq_result_df.agg(F.min("created_ts").alias("start_time"), F.max("created_ts").alias("end_time")).first()
        start_time_out = min_max_df["start_time"]
        end_time_out = min_max_df["end_time"]
		logger.info(f"No start time provided.using min-max times. start time: (start_time_out), End time: {end_time_out)")
        return start_time_out, end_time_out

# Function to compute aggregated scores based on rule execution results
def get_aggregated_scores(dq_result_df):
    pass_condition = F.col('rule_status') == 'Pass'
    fail_condition = F.col('rule_status') == 'Fail'
    not_executed_condition = F.col('rule_status') == 'Not Executed'

    overall_window = Window.partitionBy('database_name', 'table_name')

    # Group and aggregate DQ results
    dq_result_df = (
        dq_result_df.groupby('database_name', 'table_name', 'column_name')
        .agg(
            F.sum(F.when(pass_condition, 1).otherwise(0)).alias('pass_count'),
            F.sum(F.when(fail_condition, 1).otherwise(0)).alias('failed_count'),
            F.sum(F.when(not_executed_condition, 1).otherwise(0)).alias('not_executed_count'),
            F.countDistinct('rule_def_id').alias('total_rules')
        )
    )

    # Calculate pass_percentage based on conditions
    dq_result_df = dq_result_df.withColumn(
        'pass_percentage',
        F.round(
            F.when(F.upper(F.col('column name')) == 'NA',
                   F.sum(F.col('pass_count')).over(overall_window) / F.sum(F.col('total_rules')).over(overall_window))
            .otherwise(F.col('pass_count') / F.col('total_rules')) * 100,
            8
        )
    )

    return dq_result_df

# Main function to harvest data from DQ to DCM
def harvest_dq4qd(spark, from_time):
    # Load DQ, DCM, and AST hierarchy dataframes
    dq_result_df = sparkUtil.create_db_dataframe(spark, "dq", "rule result_table")
    dq_rule_table_df = sparkUtil.create_db_dataframe(spark, "dcm", "rule_table")
    dq_agg_table_df = sparkUtil.create_db_dataframe(spark, "dcm", "agg_table")
    ast_hierarchy_df = sparkUtil.create_db_dataframe(spark, "dcm", "ast_hierarchy_table")

    # Fetch the time range for DQ results
    start_time, end_time = fetch_start_end_time(dq_result_df, from_time)
    # Filter DQ results based on the time range
    dq_result_df = dq_result_df.filter((F.col("created_ts") > start_time) & (F.col("created_ts") <= end_time))

    # Deduplicate DQ rule dataframe
    dq_rule_df = dq_result_df.dropDuplicates(['table_name', 'database_name', 'column_name', 'rule_def_id'])
    # Compute aggregated scores for DQ results
    dq_result_df = get_aggregated_scores(dq_result_df)
    # Count the number of records received from DQ
    num_of_records_fromDQ = dq_result_df.count()
	
	logger.info(f"num of records in dq4qd table where created_ts > {start_time} and <= {end_time} are: {num_of_records_fromDQ}")
	assets_present_in_dom_count = 0

	if num_of_records_fromDQ > 0:
		# Generate conditions for AST hierarchy
		database_condition, table_condition, column_condition = helper.get_ast_hierarchy_conditions(ast_hierarchy_df)
		ast_hierarchy_df = (
			ast_hierarchy_df.withColumn('database_name', database_condition)
			.withColumn('table_name', table_condition)
			.withColumn('column_name', column_condition)
		)

		# Join DQ results with AST hierarchy
		dq_result_wth_ast_id_df = dq_result_df.join(ast_hierarchy_df,
													  (F.upper(dq_result_df['database name']) ==
													   F.upper(ast_hierarchy_df['database name'])) &
													  (F.upper(dq_result_df['table name']) ==
													   F.upper(ast_hierarchy_df['table name'])) &
													  (F.upper(dq_result_df['column_name']) ==
													   F.upper(ast_hierarchy_df['column name'])),
													  'inner')

		# Count the number of assets present in DCM
		assets_present_in_dcm_count = dq_result_with_ast_id_df.count()

		# Select relevant columns for DQ aggregated scores
		dq_agg_df = dq_result_with_ast_id_df.select(
			F.col('AST ID'),
			F.col('pass_count'),
			F.col('failed_count'),
			F.col('not executed count'),
			F.col('total rules'),
			F.col('pass_percentage')
		)

		# Count the total number of records from DQ aggregated scores
		total_count_from_dq = dq_agg_df.count()
		# Count the number of rules to update or insert
		count_to_update_or_insert = dq_rule_df.count()

		# Load configuration properties for update query
		update_config = load_properties("dcm")
		update_query_template = update_config["dq_agg_update_query"]["query"]

		# Create column mapping for update query
		column_mapping = {key: value for key, value in update_config["dq_agg_update_query"].items() if key != "query"}
		# Join DQ aggregated scores with existing records in DCM
		updates_dq_agg_df = dq_agg_df.join(dq_agg_table_df, 'ast_id', 'inner')
		# Coalesce partitions to improve performance
		updates_dq_agg_df = updates_dq_agg_df.coalesce(2)

		# If there are records to update, execute the update
		if updates_dq_agg_df.count() > 0:
			updates_dq_agg_df = updates_dq_agg_df.toDF(*[col.lower() for col in updates_dq_agg_df.columns])

			# Execute the update query for DQ aggregated scores
			updates_dq_agg_df.foreachPartition(
				lambda iterator: sparkUtil.update_records(iterator, update_config, update_query_template, column_mapping)
			)

		# Identify new records for DQ aggregated scores
		new_records_dq_agg_df = dq_agg_df.join(updates_dq_agg_df, 'ast_id', 'left anti')
		# Coalesce partitions for better performance
		new_records_dq_agg_df = new_records_dq_agg_df.coalesce(2)

		# Generate a unique identifier for new records
		new_records_dq_agg_df = new_records_dq_agg_df.withColumn('data_qual_id', F.expr('uuid()'))

		# Write new records to the DCM aggregate table
		sparkUtil.write_dataframe_to_db(new_records_dq_agg_df, "dcm", "agg_table")

		# Join DQ rule dataframe with AST hierarchy dataframe
		final_dq_rule_df = dq_rule_with_ast_id_df.join(new_records_dq_agg_df, 'ast_id', 'inner').select(
			F.col('data_qual_id'),
			F.col('RULE_DEF_ID'),
			F.col('RULE NAME')
		).union(updates_dq_rule_df)

		# Join updates DQ rule dataframe with existing records in DCM
		updates_dq_rule_df = dq_rule_with_ast_id_df.join(dq_rule_table_df, 'data_qual_id', 'inner')
		updates_dq_rule_df = updates_dq_rule_df.toDF(*[col.lower() for col in updates_dq_rule_df.columns])

		# Load configuration properties for delete query
		update_config = load_properties("dcm")
		update_query_template = update_config["dq_rule_delete_query"]["query_del"]

		# Create column mapping for delete query
		column_mapping = {key: value for key, value in update_config["dq_rule_delete_query"].items() if key != "query_del"}

		# If there are records to update, execute the delete query for DQ rules
		if updates_dq_rule_df.count() > 0:
			updates_dq_rule_df.foreachPartition(
				lambda iterator: sparkUtil.update_records(iterator, update_config, update_query_template, column_mapping)
			)

		# Identify new records for DQ rules
		new_records_dq_rule_df = dq_rule_df.join(updates_dq_rule_df, 'data_qual_id', 'left anti')

		# Generate a unique identifier for new records
		new_records_dq_rule_df = new_records_dq_rule_df.withColumn('data_qual_rule_id', F.expr('uuid()'))

		# Write new records to the DCM rule table
		sparkUtil.write_dataframe_to_db(new_records_dq_rule_df, "dcm", "rule_table")

		# Generate a unique identifier for final DQ rule records
		final_dq_rule_df = final_dq_rule_df.withColumn('data_qual_rule_id', F.expr('uuid()'))

		# Select relevant columns for final DQ rule records
		final_dq_rule_df = final_dq_rule_df.select(
			F.col("data_qual_rule_id"),
			F.col("data_qual_id"),
			F.col("rule_def_id"),
			F.col("rule_name"),
			F.lit(1111).alias("creat_btch_id"),
			F.lit("DQ4QD").alias("creat_nm"),
			F.current_timestamp().alias("creat_ts"),
			F.lit("").alias("updt_btch_id"),
			F.lit("").alias("updt_nm"),
			F.lit("").alias("updt_ts")
		)

		# Show the final DQ rule dataframe
		final_dq_rule_df.show()

    # Generate statistics for the job
    stats = {"receivedFromDQ": num_of_records_fromDQ, "insertedIntoDCM": assets_present_in_dcm_count}
    json_stats = json.dumps(stats)

    # Return start time, end time, and job statistics in JSON format
    return start_time, end_time, json_stats
