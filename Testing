"""
Function: harvest_technical_metadata

Description:
This function performs technical metadata harvesting using PySpark. It leverages Spark to extract metadata details from various sources such as JSON files, database tables, and views.

Parameters:
- spark: PySpark session used for distributed data processing.

Steps:
1. Load JSON configuration properties for the data configuration manager (DCM).
2. Extract the JSON file path from the configuration.
3. Create a Spark DataFrame from the JSON file using the provided file path.
4. Retrieve table details and column details from the JSON DataFrame.
5. Create a DataFrame specific to application functions from a predefined database query.
6. Capture application function details and remove the 'ait_name' column from the resulting DataFrame.
7. Create DataFrames for asset types and clusters from specified database queries.
8. Persist the asset type IDs DataFrame for later use.
9. Identify the asset type ID for the "DATABASE" type.
10. Harvest cluster details using the identified database asset type ID.
11. Identify the asset type ID for the "SCHEMA" type.
12. Harvest schema details using the identified schema asset type ID, cluster details, and hierarchy data.
13. Harvest table details, and persist the resulting DataFrame.
14. Harvest column details using asset type IDs, schema table details, and hierarchy data.
15. Identify virtual views in the table details DataFrame.
16. Add relationships for virtual views using auxiliary functions and provided parameters.
17. Unpersist DataFrames to release resources.

Note:
- Adjust the function based on your specific use case and data model.
- Ensure the necessary functions like 'load_properties', 'create_json_dataframe', 'get_table_details', etc., are defined or imported.
- Handle any specific database queries and processing logic accordingly.
- Check the integrity of the 'add_view_rel' function and its parameters.
- Documented function steps provide a high-level overview; add comments within the function for detailed explanations.
"""
def harvest_technical_metadata(spark):
    # Function implementation...
====================================================================

"""
Function: load_properties

Description:
This function loads properties from one or more property files using the configparser module. The property files should be in the same directory as the script invoking this function.

Parameters:
- *filenames: Variable number of filenames (without extensions) to load properties from.

Returns:
A dictionary containing properties from the specified property files, organized by sections.

Example:
If invoked with `load_properties('file1', 'file2')`, it will read 'file1.properties' and 'file2.properties', and return a dictionary with properties from both files.

Note:
- Ensure the property files are in the same directory as the script calling this function.
- The function reads properties organized by sections in the property files.
- Each section's properties are stored in the dictionary under the section name.
- If there are duplicate section names across files, the properties from later files overwrite those from earlier ones.
- If a property is present in multiple sections, the last occurrence takes precedence.

Usage:
props_dict = load_properties('file1', 'file2')
"""

import os
import configparser

def load_properties(*filenames):
    """
    Load properties from one or more property files.

    :param *filenames: Variable number of filenames (without extensions).
    :return: A dictionary containing properties from the specified property files.
    """
    # Get the directory path of the script
    dir_path = os.path.dirname(os.path.realpath(__file__))

    # Initialize an empty dictionary to store properties
    props = {}

    # Iterate through each specified filename
    for filename in filenames:
        # Create the full path to the property file
        full_path = os.path.join(os.path.dirname(__file__), f"{filename}.properties")

        # Create a ConfigParser instance and read the property file
        config = configparser.ConfigParser()
        config.read(full_path)

        # Iterate through sections in the property file and store properties in the dictionary
        for section in config.sections():
            props[section] = dict(config[section])

    # Return the dictionary containing all loaded properties
    return props
==============================================================================================================

"""
Function: get_table_details

Description:
This function processes a PySpark DataFrame representing JSON data containing table details. It extracts relevant information, transforms the data, and adds new columns to create a comprehensive table details DataFrame.

Parameters:
- json_df: PySpark DataFrame containing JSON data with table details.

Returns:
A new PySpark DataFrame enriched with additional columns representing various table attributes.

Note:
- The function assumes the structure of the input DataFrame with nested fields, adjust accordingly if needed.
- Some columns are conditionally populated based on the value of other columns.

Usage:
result_df = get_table_details(json_df)
"""

from pyspark.sql import functions as F

def get_table_details(json_df):
    """
    Process JSON data containing table details and create an enriched DataFrame.

    :param json_df: PySpark DataFrame containing JSON data with table details.
    :return: A new PySpark DataFrame with additional columns representing various table attributes.
    """
    result_df = (
        json_df
        .withColumn("databaseName", F.col("tableInfo.Database:"))
        .withColumn("cluster_info", F.col("tableInfo.cluster_info"))
        .withColumn("OwnerType", F.col("tableInfo.OwnerType:"))
        .withColumn("Owner", F.col("tableInfo.Owner:"))
        .withColumn("CreateTime", F.col("tableInfo.CreateTime:"))
        .withColumn("LastAccessTime", F.col("tableInfo.LastAccessTime:"))
        .withColumn("Retention", F.col("tableInfo.Retention:"))
        .withColumn("TableType", F.col("tableInfo.Table Type:"))
        .withColumn("TableParameters", F.col("tableInfo.Table Parameters:"))
        .withColumn("SerDeLib", F.col("storageInformation.SerDe Library:"))
        .withColumn("InputFormat", F.col("storageInformation.InputFormat:"))
        .withColumn("OutputFormat", F.col("storageInformation.OutputFormat:"))
        .withColumn("Compressed", F.when(F.col("storageInformation.Compressed:") == F.lit("No"), 'N').otherwise(F.lit("Y")))
        .withColumn("NumBuckets", F.col("storageInformation.Num Buckets:"))
        .withColumn("BucketColumns", F.col("storageInformation.Bucket Columns:"))
        .withColumn("SortColumns", F.col("storageInformation.Sort Columns:"))
        .withColumn("vw_def", F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("EXTERNAL"), None)
                    .otherwise(F.col("viewInformation.View Expanded Text:")))
        .withColumn("location", F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("VIRTUAL_VIEW"), None)
                    .otherwise(F.col("storageInformation.location")))
        .withColumn("ait_name", F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("VIRTUAL_VIEW"),
                                       (F.select("OwnerType", "Owner", "CreateTime", "LastAccessTime", "Retention")
                                        .otherwise(F.split(F.col("storageInformation.location"), '/')[4]))
                                       | F.regexp_replace(F.split(F.col("tableInfo.Database:"), '_')[0], '^', ''))
        )
    )
    return result_df

==================================================================================================================

"""
Function: get_column_details

Description:
This function processes a PySpark DataFrame representing JSON data containing column details. It selects and transforms relevant information to create a DataFrame specifically for asset columns.

Parameters:
- json_df: PySpark DataFrame containing JSON data with column details.

Returns:
A new PySpark DataFrame with columns exploded from the original JSON structure, along with additional columns representing table and database information.

Note:
- The function assumes the structure of the input DataFrame with nested fields, adjust accordingly if needed.
- The column "ait_name" is commented out in the current implementation; uncomment and adapt if required.

Usage:
column_details_df = get_column_details(json_df)
"""

from pyspark.sql import functions as F

def get_column_details(json_df):
    """
    Process JSON data containing column details and create a DataFrame specifically for asset columns.

    :param json_df: PySpark DataFrame containing JSON data with column details.
    :return: A new PySpark DataFrame with columns exploded from the original JSON structure, along with additional columns representing table and database information.
    """
    asset_columns_df = (
        json_df
        .select(
            F.explode(F.col("columns")).alias("columns"),
            "tableName",
            F.col("tableInfo.Database:").alias("databaseName"),
            F.col("tableInfo.cluster_info").alias("cluster_info")
            # Uncomment and adapt the following lines if needed
            # , F.col("tableInfo.Table Type:").alias("TableType")
            # , F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("VIRTUAL_VIEW"),
            #          F.lit("VIRTUAL_VIEW"))
            #   .otherwise(F.regexp_replace(F.split(F.col("tableInfo.Database:"), '_')[0], r'^.', ''))
            # .alias("ait_name")
        )
    )

    # Return the final DataFrame
    return asset_columns_df.select("columns.*", "tableName", "databaseName", "cluster_info")

# Example usage:
# column_details_df = get_column_details(json_df)


==========================================================================================================

"""
Function: create_cols_specific_db_dataframe

Description:
This function creates a PySpark DataFrame by executing a specific column-related query on a database using JDBC. It relies on configuration details provided in a property file.

Parameters:
- spark_session: PySpark session used for distributed data processing.
- database_config: Name of the property file containing database configuration details.
- query: Key to identify the specific column-related query in the property file.

Returns:
A PySpark DataFrame containing the result of the specified column-related query.

Note:
- The function uses JDBC to connect to the database and execute the query.
- Ensure the property file structure is appropriate for extracting necessary database configuration details.
- The property file should include a section named "col_specific_queries" with keys corresponding to different queries.

Usage:
df = create_cols_specific_db_dataframe(spark_session, "database_config.properties", "specific_query_key")
"""

from pyspark.sql import SparkSession

def create_cols_specific_db_dataframe(spark_session: SparkSession, database_config: str, query: str):
    """
    Create a PySpark DataFrame by executing a specific column-related query on a database using JDBC.

    :param spark_session: PySpark session used for distributed data processing.
    :param database_config: Name of the property file containing database configuration details.
    :param query: Key to identify the specific column-related query in the property file.
    :return: A PySpark DataFrame containing the result of the specified column-related query.
    """
    # Load database configuration from the property file
    db_config = load_properties(database_config)

    # Print the query for debugging purposes
    print(db_config["col_specific_queries"][query])

    # Read data from the database using JDBC
    dataframe = (
        spark_session.read
        .format("jdbc")
        .option("url", db_config["db"]["url"])
        .option("query", db_config["col_specific_queries"][query])
        .option("user", db_config["db"]["user"])
        .option("password", db_config["db"]["password"])
        .option("driver", db_config["db"]["driver"])
        .load()
    )

    # Return the resulting DataFrame
    return dataframe

# Example usage:
# df = create_cols_specific_db_dataframe(spark, "database_config.properties", "specific_query_key")


=====================================================================================================================

"""
Function: captured_ait_details

Description:
This function captures AIT (Application Interface Table) details by joining information from multiple DataFrames and handling unknown AIT cases.

Parameters:
- spark: PySpark session used for distributed data processing.
- json_table_cols_details_df: PySpark DataFrame containing JSON data with table and column details.
- appln_df: PySpark DataFrame containing application details.

Returns:
A PySpark DataFrame with enriched AIT details.

Note:
- The function assumes certain columns and conditions, adjust accordingly if needed.
- It uses a specific query to retrieve information for unknown AITs.
- Ensure the property file structure is appropriate for extracting necessary details.
- Handle any additional conditions or customizations based on your specific use case.

Usage:
ait_details_final_df = captured_ait_details(spark, json_table_cols_details_df, appln_df)
"""

from pyspark.sql import functions as F

def captured_ait_details(spark, json_table_cols_details_df, appln_df):
    """
    Capture AIT details by joining information from multiple DataFrames and handling unknown AIT cases.

    :param spark: PySpark session used for distributed data processing.
    :param json_table_cols_details_df: PySpark DataFrame containing JSON data with table and column details.
    :param appln_df: PySpark DataFrame containing application details.
    :return: A PySpark DataFrame with enriched AIT details.
    """
    # Query to retrieve details for unknown AITs
    unknown_ait_for_null_df = sparkUtil.create_cols_specific_db_dataframe(spark, "dcm", "unknown_ait_id_query")

    # Extract the unknown AIT for null views
    unknown_ait_for_null_view = unknown_ait_for_null_df.select('table_ait_id').first()

    # Join JSON table columns details with application DataFrame
    ait_details_df = json_table_cols_details_df.join(appln_df, (F.upper(json_table_cols_details_df['ait_name'])
                                                               == F.upper(appln_df['ait_shrt_nm'])), 'left').drop('ait_shrt_nm')

    # Enrich the DataFrame with AIT details
    ait_details_final_df = (
        ait_details_df
        .withColumn('ait_id', F.when((F.col('TableType') == F.lit('VIRTUAL_VIEW')) &
                                     (F.col('AIT_ID').isNull()),
                                     F.lit(unknown_ait_for_null_view['table_ait_id']))
                    .otherwise(F.col('table_ait_id'))
        )
        .drop('table_ait_id')  # Drop the original AIT_ID column
        # Add additional conditions or transformations if needed
    )

    # Return the final DataFrame with enriched AIT details
    return ait_details_final_df

# Example usage:
# ait_details_final_df = captured_ait_details(spark, json_table_cols_details_df, appln_df)


==================================================================================================================

"""
Function: create_db_dataframe

Description:
This function creates a PySpark DataFrame by reading data from a database table using JDBC. It relies on configuration details provided in a property file.

Parameters:
- spark_session: PySpark session used for distributed data processing.
- database_config: Name of the property file containing database configuration details.
- table_name: Name of the table from which data will be read.

Returns:
A PySpark DataFrame containing the data from the specified database table.

Note:
- The function uses JDBC to connect to the database and read data from the specified table.
- Ensure the property file structure is appropriate for extracting necessary database configuration details.

Usage:
df = create_db_dataframe(spark_session, "database_config.properties", "table_name")
"""

from pyspark.sql import SparkSession

def create_db_dataframe(spark_session: SparkSession, database_config: str, table_name: str):
    """
    Create a PySpark DataFrame by reading data from a database table using JDBC.

    :param spark_session: PySpark session used for distributed data processing.
    :param database_config: Name of the property file containing database configuration details.
    :param table_name: Name of the table from which data will be read.
    :return: A PySpark DataFrame containing the data from the specified database table.
    """
    # Load database configuration from the property file
    db_config = load_properties(database_config)

    # Print the table name for debugging purposes
    print(db_config["table"][table_name])

    # Read data from the database table using JDBC
    dataframe = (
        spark_session.read
        .format("jdbc")
        .option("url", db_config["db"]["url"])
        .option("dbtable", db_config["table"][table_name])
        .option("user", db_config["db"]["user"])
        .option("password", db_config["db"]["password"])
        .option("driver", db_config["db"]["driver"])
        .load()
    )

    # Return the resulting DataFrame
    return dataframe

# Example usage:
# df = create_db_dataframe(spark, "database_config.properties", "table_name")


=======================================================================================================================

"""
Function: create_cols_specific_db_dataframe

Description:
This function creates a PySpark DataFrame by executing a specific query on a database using JDBC. It relies on configuration details provided in a property file.

Parameters:
- spark_session: PySpark session used for distributed data processing.
- database_config: Name of the property file containing database configuration details.
- query: Key to identify the specific query in the property file.

Returns:
A PySpark DataFrame containing the result of the specified query.

Note:
- The function uses JDBC to connect to the database and execute the query.
- Ensure the property file structure is appropriate for extracting necessary database configuration details.
- The property file should include a section named "col_specific_queries" with keys corresponding to different queries.

Usage:
df = create_cols_specific_db_dataframe(spark_session, "database_config.properties", "specific_query_key")
"""

from pyspark.sql import SparkSession

def create_cols_specific_db_dataframe(spark_session: SparkSession, database_config: str, query: str):
    """
    Create a PySpark DataFrame by executing a specific query on a database using JDBC.

    :param spark_session: PySpark session used for distributed data processing.
    :param database_config: Name of the property file containing database configuration details.
    :param query: Key to identify the specific query in the property file.
    :return: A PySpark DataFrame containing the result of the specified query.
    """
    # Load database configuration from the property file
    db_config = load_properties(database_config)

    # Print the query for debugging purposes
    print(db_config["col_specific_queries"][query])

    # Read data from the database using JDBC
    dataframe = (
        spark_session.read
        .format("jdbc")
        .option("url", db_config["db"]["url"])
        .option("query", db_config["col_specific_queries"][query])
        .option("user", db_config["db"]["user"])
        .option("password", db_config["db"]["password"])
        .option("driver", db_config["db"]["driver"])
        .load()
    )

    # Return the resulting DataFrame
    return dataframe

# Example usage:
# df = create_cols_specific_db_dataframe(spark, "database_config.properties", "specific_query_key")


===========================================================================================================

"""
Function: get_asset_type_id

Description:
This function retrieves the asset type ID for a given asset type from a DataFrame containing asset type IDs.

Parameters:
- ast_typ_ids_df: PySpark DataFrame containing asset type IDs.
- ast_type: Asset type for which the ID needs to be retrieved.

Returns:
The asset type ID corresponding to the provided asset type.

Note:
- Ensure the DataFrame structure aligns with expectations.
- The function assumes the presence of columns 'ALL_AST_TYP_CD' and 'ALL_AST_TYP_ID' in the DataFrame.

Usage:
ast_type_id = get_asset_type_id(ast_typ_ids_df, "example_asset_type")
"""

def get_asset_type_id(ast_typ_ids_df, ast_type):
    """
    Retrieve the asset type ID for a given asset type from a DataFrame containing asset type IDs.

    :param ast_typ_ids_df: PySpark DataFrame containing asset type IDs.
    :param ast_type: Asset type for which the ID needs to be retrieved.
    :return: The asset type ID corresponding to the provided asset type.
    """
    # Filter the DataFrame to find the asset type ID
    filter_ast_typ_df = ast_typ_ids_df.filter(ast_typ_ids_df.ALL_AST_TYP_CD == ast_type)
    
    # Extract the asset type ID from the filtered DataFrame
    ast_typ_id = filter_ast_typ_df.select('ALL_AST_TYP_ID').first()
    
    # Return the asset type ID as a Python value
    return ast_typ_id['ALL_AST_TYP_ID']

# Example usage:
# ast_type_id = get_asset_type_id(ast_typ_ids_df, "example_asset_type")

============================================================================================================

"""
Function: harvest_cluster_details

Description:
This function harvests cluster details from hierarchical data and table details based on provided parameters.

Parameters:
- db_ast_typ_id_for_insert: Asset type ID for the cluster to be inserted into.
- ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
- table_details_df: PySpark DataFrame containing details about tables.

Returns:
A PySpark DataFrame containing harvested cluster details.

Note:
- The function assumes the presence of certain columns in the input DataFrames.
- Ensure the DataFrame structures align with expectations.

Usage:
harvested_cluster_df = harvest_cluster_details("example_ast_typ_id", ast_hierarchy_df, table_details_df)
"""

from pyspark.sql import functions as F

def harvest_cluster_details(db_ast_typ_id_for_insert, ast_hierarchy_df, table_details_df):
    """
    Harvest cluster details from hierarchical data and table details.

    :param db_ast_typ_id_for_insert: Asset type ID for the cluster to be inserted into.
    :param ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
    :param table_details_df: PySpark DataFrame containing details about tables.
    :return: A PySpark DataFrame containing harvested cluster details.
    """
    # Select relevant columns from the hierarchical DataFrame
    ast_cluster_hierarchy_df = ast_hierarchy_df.select(
        F.col('CLUSTER_ASSET_ID'),
        F.col('CLUSTER_ASSET_TYPE_ID'),
        F.col('CLUSTER_NAME').alias('AST_NM')
    ).distinct()

    # Execute cluster-level operations using a helper function (cluster_level_exec)
    ast_cluster_df = cluster_level_exec(table_details_df, ast_cluster_hierarchy_df, db_ast_typ_id_for_insert)

    # Return the resulting DataFrame containing harvested cluster details
    return ast_cluster_df

# Example usage:
# harvested_cluster_df = harvest_cluster_details("example_ast_typ_id", ast_hierarchy_df, table_details_df)


===================================================================================================================

"""
Function: harvest_schema_details

Description:
This function harvests schema details from hierarchical data, cluster details, and table details based on provided parameters.

Parameters:
- schm_ast_typ_id_for_insert: Asset type ID for the schema to be inserted into.
- ast_cluster_df: PySpark DataFrame containing harvested cluster details.
- ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
- table_details_df: PySpark DataFrame containing details about tables.

Returns:
A PySpark DataFrame containing harvested schema details.

Note:
- The function assumes the presence of certain columns in the input DataFrames.
- Ensure the DataFrame structures align with expectations.

Usage:
harvested_schema_df = harvest_schema_details("example_schm_ast_typ_id", ast_cluster_df, ast_hierarchy_df, table_details_df)
"""

from pyspark.sql import functions as F

def harvest_schema_details(schm_ast_typ_id_for_insert, ast_cluster_df, ast_hierarchy_df, table_details_df):
    """
    Harvest schema details from hierarchical data, cluster details, and table details.

    :param schm_ast_typ_id_for_insert: Asset type ID for the schema to be inserted into.
    :param ast_cluster_df: PySpark DataFrame containing harvested cluster details.
    :param ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
    :param table_details_df: PySpark DataFrame containing details about tables.
    :return: A PySpark DataFrame containing harvested schema details.
    """
    # Select relevant columns from the hierarchical DataFrame
    ast_schm_hierarchy_df = ast_hierarchy_df.select(
        'CLUSTER_NAME',
        F.col('SCHEMA_ASSET_ID').alias('ast_dd_prime'),
        'SCHEMA_ASSET_TYPE_ID',
        F.col('SCHEMA_NAME').alias('AST_NM')
    ).distinct()

    # Execute schema-level operations using a helper function (schema_level_exec)
    ast_schm_df = schema_level_exec(table_details_df, ast_schm_hierarchy_df, ast_cluster_df, schm_ast_typ_id_for_insert)

    # Return the resulting DataFrame containing harvested schema details
    return ast_schm_df

# Example usage:
# harvested_schema_df = harvest_schema_details("example_schm_ast_typ_id", ast_cluster_df, ast_hierarchy_df, table_details_df)


====================================================================================================================================

"""
Function: harvest_table_details

Description:
This function harvests table details from hierarchical data, schema details, and table details based on provided parameters.

Parameters:
- ast_typ_ids_df: PySpark DataFrame containing asset type IDs.
- ast_schm_df: PySpark DataFrame containing harvested schema details.
- ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
- table_details_df: PySpark DataFrame containing details about tables.

Returns:
Two PySpark DataFrames - one containing harvested table details for a schema, and another containing tables to be inserted.

Note:
- The function assumes the presence of certain columns in the input DataFrames.
- Ensure the DataFrame structures align with expectations.

Usage:
harvested_tbl_df, insert_tbls_schm_df = harvest_table_details(ast_typ_ids_df, ast_schm_df, ast_hierarchy_df, table_details_df)
"""

from pyspark.sql import functions as F

def harvest_table_details(ast_typ_ids_df, ast_schm_df, ast_hierarchy_df, table_details_df):
    """
    Harvest table details from hierarchical data, schema details, and table details.

    :param ast_typ_ids_df: PySpark DataFrame containing asset type IDs.
    :param ast_schm_df: PySpark DataFrame containing harvested schema details.
    :param ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
    :param table_details_df: PySpark DataFrame containing details about tables.
    :return: Two PySpark DataFrames - one containing harvested table details for a schema, and another containing tables to be inserted.
    """
    # Select relevant columns from the hierarchical DataFrame
    ast_tables_hierarchy_df = ast_hierarchy_df.select(
        'CLUSTER_NAME',
        'SCHEMA_NAME',
        F.col('TABLE_ASSET_ID').alias('ast_id'),
        'TABLE_ASSET_TYPE_ID',
        F.col('TABLE_ASSET_NAME').alias('AST_NM')
    ).distinct()

    # Execute table-level operations using a helper function (tables_level_exec)
    schm_tbl_df, insert_tbls_schm_df = tables_level_exec(table_details_df, ast_tables_hierarchy_df, ast_schm_df, ast_typ_ids_df)

    # Return the resulting DataFrames containing harvested table details
    return schm_tbl_df, insert_tbls_schm_df

# Example usage:
# harvested_tbl_df, insert_tbls_schm_df = harvest_table_details(ast_typ_ids_df, ast_schm_df, ast_hierarchy_df, table_details_df)


====================================================================================================================

"""
Function: harvest_columns_details

Description:
This function harvests column details from hierarchical data, table details, and column details based on provided parameters.

Parameters:
- ast_typ_ids_df: PySpark DataFrame containing asset type IDs.
- schm_tbl_df: PySpark DataFrame containing harvested table details.
- ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
- col_details_df: PySpark DataFrame containing details about columns.

Note:
- The function assumes the presence of certain columns in the input DataFrames.
- Ensure the DataFrame structures align with expectations.

Usage:
harvest_columns_details(ast_typ_ids_df, schm_tbl_df, ast_hierarchy_df, col_details_df)
"""

from pyspark.sql import functions as F

def harvest_columns_details(ast_typ_ids_df, schm_tbl_df, ast_hierarchy_df, col_details_df):
    """
    Harvest column details from hierarchical data, table details, and column details.

    :param ast_typ_ids_df: PySpark DataFrame containing asset type IDs.
    :param schm_tbl_df: PySpark DataFrame containing harvested table details.
    :param ast_hierarchy_df: PySpark DataFrame containing hierarchical data.
    :param col_details_df: PySpark DataFrame containing details about columns.
    """
    # Get the asset type ID for columns
    col_ast_typ_id_for_insert = get_asset_type_id(ast_typ_ids_df, "COLUMN")

    # Select relevant columns from the hierarchical DataFrame
    ast_cols_hierarchy_df = ast_hierarchy_df.select(
        'CLUSTER_NAME',
        'SCHEMA_NAME',
        'TABLE_ASSET_NAME',
        F.col('COLUMN_ASSET_TYPE_ID'),
        F.col('COLUMN_ASSET_ID').alias('ast_id'),
        F.col('COLUMN_ASSET_NAME').alias('AST_NM')
    )

    # Execute column-level operations using a helper function (columns_level_exec)
    columns_level_exec(col_details_df, ast_cols_hierarchy_df, schm_tbl_df, col_ast_typ_id_for_insert)

# Example usage:
# harvest_columns_details(ast_typ_ids_df, schm_tbl_df, ast_hierarchy_df, col_details_df)


==========================================================================================================

"""
Function: cluster_level_exec

Description:
This function executes cluster-level operations based on provided parameters.

Parameters:
- table_details_df: PySpark DataFrame containing details about tables.
- ast_cluster_hierarchy_df: PySpark DataFrame containing hierarchical data for clusters.
- db_ast_typ_id_for_insert: Asset type ID for the cluster to be inserted into.

Returns:
A PySpark DataFrame containing cluster details.

Note:
- The function assumes the presence of certain columns in the input DataFrames.
- Ensure the DataFrame structures align with expectations.

Usage:
cluster_details_df = cluster_level_exec(table_details_df, ast_cluster_hierarchy_df, "example_db_ast_typ_id_for_insert")
"""

from pyspark.sql import functions as F

def cluster_level_exec(table_details_df, ast_cluster_hierarchy_df, db_ast_typ_id_for_insert):
    """
    Execute cluster-level operations based on provided parameters.

    :param table_details_df: PySpark DataFrame containing details about tables.
    :param ast_cluster_hierarchy_df: PySpark DataFrame containing hierarchical data for clusters.
    :param db_ast_typ_id_for_insert: Asset type ID for the cluster to be inserted into.
    :return: A PySpark DataFrame containing cluster details.
    """
    print("\n-cluster level execution====\n")

    # Select distinct cluster information from the table details DataFrame
    cluster_to_join = table_details_df.select("cluster_info").distinct()

    # Join cluster information with the provided cluster hierarchy DataFrame
    ast_cluster_join_df = cluster_to_join.join(
        ast_cluster_hierarchy_df,
        (F.upper(cluster_to_join['cluster_info']) == F.upper(ast_cluster_hierarchy_df['AST_NM'])),
        'left'
    )

    # Get DataFrames for update and insert operations
    update_cluster_df, insert_cluster_details_df = get_updt_insert_df(ast_cluster_join_df)

    # Persist the updated cluster DataFrame
    new_cluster_df = update_cluster_df.persist()

    # Create DataFrame for inserting new cluster information into the asset table
    insert_cluster_in_ast_tbl_df = new_cluster_df.select(
        F.col("cluster_ast_id").alias("ast_id"),
        F.col("cluster_ast_typ_id").alias("ast_typ_id"),
        F.col("cluster_nm").alias("ast_nm"),
        F.lit(111).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    )

    # Write the DataFrame into the asset table
    write_df_in_assets(insert_cluster_in_ast_tbl_df, "dcm", "asset_table")

    # Create DataFrame for inserting new cluster information into the asset database table
    insert_cluster_in_ast_db_df = new_cluster_df.select(
        F.col("db_id"),
        F.col("cluster_ast_id").alias("ast_id"),
        F.col("cluster_nm").alias("db_nm"),
        F.lit("Hive").alias("db_typ_nm"),
        F.col("cluster_nm").alias("host_nm"),
        F.lit(1).alias("ast_aprv_fl"),
        F.lit("active").alias("status"),
        F.lit(1).alias("active_fl"),
        F.lit(1001202312221607).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    )

    # Write the DataFrame into the asset database table
    write_df_in_assets(insert_cluster_in_ast_db_df, "dcm", "asset_database")

    # Select necessary columns for update and union DataFrames
    update_cluster_details_df = new_cluster_df.select(
        F.col("cluster_info").alias("cluster_nm"),
        F.col("CLUSTER_ASSET_ID").alias("cluster_ast_id"),
        F.col("CLUSTER_ASSET_TYPE_ID").alias("cluster_ast_typ_id")
    )

    insert_cluster_union_df = new_cluster_df.select('cluster_nm', 'cluster_ast_id', 'cluster_ast_typ_id')

    # Create the final DataFrame by union of update and insert DataFrames
    update_cluster_union_df = update_cluster_details_df.filter(update_cluster_details_df.cluster_ast_id.isNotNull())
    cluster_details_df = update_cluster_union_df.union(insert_cluster_union_df).distinct()

    # Unpersist the temporary DataFrames
    update_cluster_df.unpersist()
    insert_cluster_details_df.unpersist()

    return cluster_details_df

# Example usage:
# cluster_details_df = cluster_level_exec(table_details_df, ast_cluster_hierarchy_df, "example_db_ast_typ_id_for_insert")

======================================================================================================================
