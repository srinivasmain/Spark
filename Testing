Open sidebar
ChatGPT 3.5

User
from dem.utilities import sparkDBUtility as sparkUtil
from dem.utilities import helperUtils as helper
import pyspark.sql.functions as F
from dem.properties import load_properties


def harvest_atlas(spark):
    json_config = load_properties("dam")
    json_filepath = json_config["read_json"]["json_file_path"]
    json_df = sparkUtil.create_json_dataframe(spark, json_filepath)
    
    json_df = json_df.select(F.explode(F.col("files")).alias("files")).select("files.*")
    
    # json_df.show()

    json_table_details_df = get_table_details(json_df)
    col_details_df = get_column_details(json_df)

    appln_df = sparkUtil.create_cols_specific_db_dataframe(spark, "dem", "appln_tbl_query")
    table_details_df = captured_ait_details(spark, json_table_details_df, appln_df).drop('ait_name')

    table_details_df.show()

    # col_details_df = captured_ait_details(spark, json_col_details_df, appln_df).drop('ait_name')
    
    col_details_df.show()

    read_ast_hierarchy_df = sparkUtil.create_db_dataframe(spark, "dem", "ast_hierarchy_table")

    database_condition, table_condition, column_condition = helper.get_ast_hierarchy_conditions(read_ast_hierarchy_df)
    ast_hierarchy_df = (read_ast_hierarchy_df.withColumn('database_name', database_condition)
                      .withColumn('table_name', table_condition)
                      .withColumn('column_name', column_condition)
                      )

    ast_cluster_hierarchy_df = ast_hierarchy_df.filter(ast_hierarchy_df.AST_TYP_CD == 'DATABASE')
    ast_cluster_df = cluster_level_exec(spark, table_details_df, ast_cluster_hierarchy_df)

    ast_schm_hierarchy_df = ast_hierarchy_df.filter(ast_hierarchy_df.AST_TYP_CD == 'SCHEMA')
    ast_schm_df = schema_level_exec(spark, table_details_df, ast_schm_hierarchy_df, ast_cluster_df)

    ast_tables_hierarchy_df = ast_hierarchy_df.filter((ast_hierarchy_df.AST_TYP_CD == 'ENTITY_TABLE') & (ast_hierarchy_df.database_name.isNotNull()))
    schm_tbl_df = tables_level_exec(spark, table_details_df, ast_tables_hierarchy_df, ast_schm_df)

    filter_hier_df = ast_hierarchy_df.filter((ast_hierarchy_df.AST_TYP_CD == 'COLUMN') & (ast_hierarchy_df.table_name == 'pty_rb_wk'))

    ast_cols_hierarchy_df = filter_hier_df.filter(filter_hier_df.database_name.isNotNull() & filter_hier_df.table_name.isNotNull())
    columns_level_exec(spark, col_details_df, ast_cols_hierarchy_df, schm_tbl_df)


def cluster_level_exec(spark, table_details_df, ast_hierarchy_df):
    print(" - Cluster level execution")

    db_type_schm_id = sparkUtil.create_cols_specific_db_dataframe(spark, "dcm", "db_id_query")

    db_ast_typ_id_for_insert = db_type_schm_id.select('db_ast_typ_id').collect()[0][0]

    cluster_to_join = table_details_df.select("cluster_info").distinct()
    ast_cluster_join_df = cluster_to_join.join(
        ast_hierarchy_df,
        (F.upper(cluster_to_join['cluster_info']) == F.upper(ast_hierarchy_df['ast_nm'])),
        'left'
    )

    update_cluster_df, insert_cluster_df = get_updt_insert_df(ast_cluster_join_df)

    update_cluster_df_with_ids = get_schema_and_tables_ids(spark, update_cluster_df, "ast_typ_ids", "db_id_by_ast_id_query", "db_id")

    new_cluster_df = update_cluster_df_with_ids.persist()

    insert_cluster_df = (
        insert_cluster_df.select(
            F.col("cluster_info").alias("cluster_nm"),
            F.col('ast_id').alias("cluster_ast_id"),
            F.lit(db_ast_typ_id_for_insert).alias("cluster_ast_typ_id")
        ).withColumn('db_id', F.expr('uuid()'))
    )

    insert_cluster_df.persist()

    insert_cluster_in_ast_tbl_df = insert_cluster_df.select(
        F.col("cluster_ast_id").alias("ast_id"),
        F.col("cluster_ast_typ_id").alias("ast_typ_id"),
        F.col("cluster_nm").alias("ast_nm"),
        F.col("db_id").alias("ast_db_id"),
        F.lit(111).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    )

    # insert_cluster_in_ast_tbl_df.show()
    write_df_in_assets(insert_cluster_in_ast_tbl_df, "dcm", "asset_table")

    insert_cluster_in_ast_db_df = insert_cluster_df.select(
        'db_id',
        F.col('cluster_ast_id').alias('ast_id'),
        F.col('cluster_nm').alias("db_nm"),
        F.lit('file').alias("db_typ_nm"),
        F.col('cluster_nm').alias("host_nm"),
        F.lit('active').alias("status"),
        F.lit(0).alias("active_fl"),
        F.lit(1001202312221607).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    ).withColumn('db_id', F.expr('uuid()'))

    # insert_cluster_in_ast_db_df.show()
    write_df_in_assets(insert_cluster_in_ast_db_df, "dcm", "asset_database")

    update_cluster_details_df = new_cluster_df.select(
        F.col("cluster_info").alias("cluster_nm"),
        F.col('ast_id').alias("cluster_ast_id"),
        F.col("ast_typ_id").alias("cluster_ast_typ_id"),
    )

    insert_cluster_union_df = insert_cluster_df.select('cluster_nm', 'cluster_ast_id', 'cluster_ast_typ_id', 'db_id')

    update_cluster_details_df.filter(update_cluster_details_df.db_id.isNotNull())
    cluster_details_df = update_cluster_union_df.union(insert_cluster_union_df)

    update_cluster_union_df =

    update_cluster_df_with_ids.unpersist()

    return cluster_details_df


def schema_level_exec(spark, table_details_df, ast_hierarchy_df, ast_cluster_df):
    print("Schema level execution")

    ast_cluster_df.persist()

    ast_type_schm_id = sparkUtil.create_cols_specific_db_dataframe(spark, "dem", "schema_id_query")
    schm_ast_typ_id_for_insert = ast_type_schm_id.select('schm_ast_typ_id').collect()[0][0]

    distinct_schm = table_details_df.select('databaseName', 'cluster_info', 'ait_id', 'AIF_ID').distinct()

    schm_to_join = distinct_schm.join(
        ast_cluster_df,
        (F.upper(distinct_schm['cluster_info']) == F.upper(ast_cluster_df['cluster_nm'])),
        'left'
    ).drop('cluster_nm')

    join_df = schm_to_join.join(
        ast_hierarchy_df,
        (F.upper(schm_to_join['databaseName']) == F.upper(ast_hierarchy_df['database_name'])),
        'left'
    )

    update_schm_df, insert_schm_df = get_updt_insert_df(join_df)

    update_schm_df_with_ids = get_schema_and_tables_ids(spark, update_schm_df, "ast_typIids", "schm_id_by_abt_id_query", "schm_id")

    new_schm_df = update_schm_df_with_ids.persist()

    insert_schm_df = (
        insert_schm_df.select(
            'ait_id',
            F.col("AIF_ID").alias("aif_id"),
            'cluster_info',
            'cluster_ast_id',
            'cluster_ast_typ_id',
            F.col("databaseName").alias("schm_db_nm"),
            F.col('ast_id').alias("schm_ast_id"),
            F.lit(schm_ast_typ_id_for_insert).alias("schm_ast_typ_id")
        ).withColumn('schm_id', F.expr('uuid()'))
    )

    insert_schm_df.persist()

    insert_schm_in_ast_tbl_df = insert_schm_df.select(
        F.col("schm_ast_id").alias("ast_id"),
        "ait_id",
        "aif_id",
        F.col("schm_ast_typ_id").alias("ast_typ_id"),
        F.col("schm_db_nm").alias("ast_nm"),
        F.col("schm_id").alias("ast_schm_id"),
        F.lit(111).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    )

    write_df_in_assets(insert_schm_in_ast_tbl_df, "dcm", "asset_table")

    insert_schm_rel_df = insert_schm_df.withColumn('rel_id', F.expr('uuid()'))
    insert_ast_to_ast_schm_df = insert_schm_rel_df.select(
        'rel_id',
        F.col('schm_ast_id').alias('trgt_ast_id'),
        F.col('cluster_ast_id').alias('src_ast_id'),
        F.col('schm_ast_typ_id').alias("trgt_ast_typ_id"),
        F.col('cluster_ast_typ_id').alias("src_ast_typ_id"),
        F.lit('H').alias('ast_rel_typ_cd'),
        F.lit(111).alias('creat_btch_id'),
        F.lit('zkgu3t8').alias('creat_nm'),
        F.current_timestamp().alias('creat_ts')
    )

    write_df_in_assets(insert_ast_to_ast_schm_df, "dcm", "asset_relationship")

    insert_schm_in_ast_schm_df = insert_schm_df.select(
        F.col("schm_id").alias("schm_id"),
        F.lit('active').alias("status"),
        F.lit(1).alias("act ive_fl"),
        F.lit(111).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    )

    write_df_in_assets(insert_schm_in_ast_schm_df, "dcm", "asset_schema")

    update_schm_union_df = new_schm_df.select(
        F.col("databaseName").alias("schm_db_nm"),
        F.col('ast_id').alias("schm_ast_id"),
        F.col("ast_typ_id").alias("schm_ast_typ_id"),
        'schm_id'
    )

    insert_schm_union_df = insert_schm_df.select(
        'schm_db_nm',
        'schm_ast_id',
        'schm_ast_typ_id',
        'schm_id'
    )

    update_schm_union_df = update_schm_union_df.filter(update_schm_union_df.schm_ast_id.isNotNull())

    schema_df = update_schm_union_df.union(insert_schm_union_df)

    ast_cluster_df.unpersist()
    update_schm_df_with_ids.unpersist()
    insert_schm_df.unpersist()

    return schema_df


def get_schema_and_tables_ids(spark, update_df, block_to_read, query, else_id_name_to_add):
    if update_df.count() > 0:
        print("Inside if to get ids ===")

        row_list = update_df.select('ast_id').collect()
        names_as_list = [row.ast_id for row in row_list]
        names_str = ",".join(["'{}'".format(name) for name in names_as_list])

        dcm_config = load_properties("dcm")
        config_query = dcm_config[block_to_read][query]
        query_to_get_ids = config_query.format(names_str)

        print("Query to get ids: " + query_to_get_ids)

        ids_df = sparkUtil.read_data_using_direct_query(spark, "dem", query_to_get_ids)
        update_df = update_df.join(ids_df, (F.upper(update_df['ast_id']) == F.upper(ids_df['temp_ast_id'])), 'left').drop('ast_id')

    else:
        print("=== Inside else to get ids ===")
        update_df = update_df.withColumn(else_id_name_to_add, F.lit(None))

    return update_df


def captured_ait_details(spark, json_table_cols_details_df, appln_df):
    # unknown_ait_for_null_view = appln_df.filter(F.upper('ait_nm') == 'UNKNOWN').select('table_ait_id').first()
    unknown_ait_for_null_df = sparkUtil.create_cols_specific_db_dataframe(spark, "dem", "unknown_ait_id_query")
    unknown_ait_for_null_view = unknown_ait_for_null_df.select("table_ait_id").first()

    ait_details_df = json_table_cols_details_df.join(
        appln_df,
        (F.upper(json_table_cols_details_df['ait_name']) == F.upper(appln_df['ait_shrt_nm'])),
        'left'
    ).drop('ait_shrt_nm')

    # ait_details_df.show()

    ait_details_final_df = ait_details_df.withColumn(
        'ait_id',
        F.when(
            (F.col('TableType') == F.lit('VIRTUAL_VIEW')) &
            (F.col('TABLE_AIT_ID').isNull()),
            F.lit(unknown_ait_for_null_view['table_ait_id'])
        ).otherwise(F.col('table_ait_id'))
    ).drop('TABLE_AIT_ID')

    # ait_details_final_df.show()

    return ait_details_final_df


def tables_level_exec(spark, table_details_df, ast_tables_hierarchy_df, ast_schm_df):
    print("Table level execution")

    ast_schm_df.persist()
    
    ast_type_tbl_id = sparkUtil.create_cols_specific_db_dataframe(spark, "dom", "entity_table_id_query")
    # ent_ast_typ_id_for_insert = ast_type_tbl_id.select('ent_ast_typ_id').collect()[0][0]

    update_tables_df, insert_tables_df = get_updt_insert_tables_df(table_details_df, ast_tables_hierarchy_df)

    update_tables_with_schm_df = update_tables_df.join(
        ast_schm_df,
        (F.upper(update_tables_df['databaseName']) == F.upper(ast_schm_df['schm_db_nm'])),
        'left'
    ).drop('schm_db_nm')

    ast_updt_df = update_tables_with_schm_df.persist()

    update_df_records(ast_updt_df, "dcm_ast_tbls_cols_updt_query", "ast_tbl_query")
    update_df_records(ast_updt_df, "dcm_ast_ent_tbl_updt_query", "ast_ent_table_query")

    insert_tables_with_ast_typ = insert_tables_df.withColumn(
        'join_ast_typ_code',
        F.when(F.col('TableType') == F.lit('VIRTUAL_VIEW'), F.lit('VIEW')).otherwise(F.lit('ENTITY_TABLE'))
    )

    insert_tables_final_df = insert_tables_with_ast_typ.join(
        ast_type_tbl_id,
        (F.upper(insert_tables_with_ast_typ['join_ast_typ_code']) == F.upper(ast_type_tbl_id['temp_ast_typ_cd'])),
        'left'
    ).drop('join_ast_typ_code', 'temp_ast_typ_cd')

    insert_tbls_details = (
        insert_tables_final_df.join(
            ast_schm_df,
            (F.upper(insert_tables_final_df['databaseName']) == F.upper(ast_schm_df['schm_db_nm'])),
            'left'
        ).drop('schm_db_nm')
        .withColumn('new_ast_id', F.col('ast_id'))
    )

    # insert_tables_final_df.show()

    insert_ast_tbl_tbls_df = insert_tbls_details.select(
        F.col('new_ast_id').alias("ast_id"),
        'ait_id',
        F.col('AIF_ID').alias('aif_id'),
        F.col('tbl_ast_typ_id').alias('ast_typ_id'),
        F.col('tableName').alias('ast_nm'),
        P.col('schm_id').alias('ast_achm_id'),
        F.col('tbl_id').alias('ast_entity_id'),
        F.lit(111).alias('creat_btch_id'),
        F.lit('zkgu3t8').alias('creat_nm'),
        F.current_timestamp().alias("creat_ts")
    )

    write_df_in_assets(insert_ast_tbl_tbls_df, "dem", "asset_table")

    # insert_ast_to_ast_df is used to insert new entity table relationship to schema in ast_to_ast_rel table
    insert_tbls_details = insert_tbls_details.withColumn('rel_id', F.expr('uuid()'))
    insert_ast_to_ast_df = insert_tbls_details.select(
        'rel_id',
        F.col('new_ast_id').alias('trgt_ast_id'),
        F.col('schm_ast_id').alias('src_ast_id'),
        F.col('tbl_ast_typ_id').alias("trgt_ast_typ_id"),
        F.col('schm_ast_typ_id').alias("src_ast_typ_id"),
        F.lit('H').alias('ast_rel_typ_cd'),
        P.lit(111).alias('creat_btch_id'),
        F.lit('zkgu3t8').alias('creat_nm'),
        F.current_timestamp().alias('creat_ts')
    )

    write_df_in_assets(insert_ast_to_ast_df, "dem", "asset_relationship")

    # insert_ast_ent_df is used to insert new entity as an asset in ast_entity_table table
    insert_tbls_details = insert_tbls_details.withColumn('tbl_id', F.expr('uuid()'))
    insert_ast_ent_df = insert_tbls_details.select(
        F.col("tbl_id").alias("TBL_ID"),
        F.col("new_ast_id").alias("AST_ID"),
        F.col("Compressed").alias("IS_CMPRS"),
        F.col("TableType").alias("TBL_TYPE"),
        F.col("vw_def").alias("VW_DEF"),
        P.lit(111).alias("CREAT_BTCH_ID"),
        # F.lit("400000001").alias("AIT_ID"),
        F.col("schm_id").alias("SCHM_ID"),
        F.col("tableName").alias("TBL_NM"),
        F.col("NumBuckets").alias("BUCKETING_NO"),
        P.lit("active").alias("STATUS"),
        F.lit(0).alias("ACTIVE_FL"),
        F.lit("zkgu3t8").alias("CREAT_NM"),
        F.current_timestamp().alias("CREAT_TS")
    )

    write_df_in_assets(insert_ast_ent_df, "dem", "asset_entity_table")

    update_tbls_schm_df = get_schema_and_tables_ids(spark, ast_updt_df, "ast_typ_ids", "table_id_by_ast_id_query", "tbl_id")
    update_union_tbls_schm_df = update_tbls_schm_df.select(
        F.col('ait_id').alias('appln_ait_id'),
        F.col('AIF_ID').alias('aif_id'),
        'tbl_id',
        F.col("tableName").alias("tbl_nm_for_cols"),
        F.col("ast_id").alias("tbl_ast_id"),
        F.col("ast_typ_id").alias("tbl_ast_typ_id"),
        F.col("databaseName").alias("db_nm"),
        F.col("SOURCE_ID_LVL1").alias("schm_ast_id"),
        "schm_id"
    )

    insert_tbls_schm_df = insert_tbls_details.select(
        F.col('ait_id').alias('appln_ait_id'),
        'aif_id',
        'tbl_id',
        F.col("tableName").alias("tbl_nm_for_cols"),
        F.col("new_ast_id").alias("tbl_ast_id"),
        'tbl_ast_typ_id',
        F.col("databaseName").alias("db_nm"),
        "schm_ast_id",
        "schm_id"
    )

    update_union_tbls_schm_df = update_union_tbls_schm_df.filter(update_union_tbls_schm_df.tbl_id.isNotNull())
    schm_tbl_df = update_union_tbls_schm_df.union(insert_tbls_schm_df)

    ast_schm_df.unpersist()
    ast_updt_df.unpersist()
    insert_tbls_details.unpersist()

    return schm_tbl_df

def columns_level_exec(spark, col_details_df, ast_cols_hierarchy_df, schm_tbl_df):
    print("Column level execution")

    ast_type_col_id = sparkUtil.create_cols_specific_db_dataframe(spark, "dem", "column_id_query")
    col_ast_typ_id_for_insert = ast_type_col_id.select('col_ast_typ_id').collect()[0][0]

    update_cols_df, insert_cols_df = get_updt_insert_cols_df(col_details_df, ast_cols_hierarchy_df)
    ast_updt_col_df = update_cols_df.persist()

    update_df_records(ast_updt_col_df, "dem_ast_tbls_cols_updt_query", "ast_tbl_query")
    update_df_records(ast_updt_col_df, "dcm_ast_col_tbl_updt_query", "ast_col_query")

    insert_cols_details = (
        insert_cols_df.join(
            schm_tbl_df,
            (F.upper(insert_cols_df['databaseName']) == F.upper(schm_tbl_df['db_nm'])) &
            (F.upper(insert_cols_df['tableName']) == F.upper(schm_tbl_df['tbl_nm_for_cols'])),
            'left'
        ).drop("db_nm", "tbl_nm_for_cols")
        .withColumn('cols_ast_id', F.col('ast_id'))
        .withColumn('col_ast_typ_id', F.lit(col_ast_typ_id_for_insert))
    )

    insert_cols_details.persist()

    # insert_ast_tbl_cols_df is used to insert new column as an asset in asset table
    insert_ast_tbl_cols_df = insert_cols_details.select(
        F.col('cols_ast_id').alias("ast_id"),
        F.col('appln_ait_id').alias('ait_id'),
        'aif_id',
        F.col('col_ast_typ_id').alias("ast_typ_id"),
        F.col("columnName").alias("ast_nm"),
        F.lit(111).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts"),
        F.col("schm_id").alias('ast_schm_id'),
        F.col('tbl_id').alias('ast_entity_id')
    )

    write_df_in_assets(insert_ast_tbl_cols_df, "dom", "asset_table")

    # insert_ast_to_ast_df is used to insert new column relationship to table in ast_to_ast_rel table
    insert_cols_details = insert_cols_details.withColumn('rel_id', F.expr('uuid()'))
    insert_ast_to_ast_df = insert_cols_details.select(
        'rel_id',
        F.col('cols_ast_id').alias("trgt_ast_id"),
        F.col("tbl_ast_id").alias("src_ast_id"),
        F.col("col_ast_typ_id").alias("trgt_ast_typ_id"),
        F.col("tbl_ast_typ_id").alias("src_ast_typ_id"),
        F.lit('H').alias("ast_rel_typ_cd"),
        F.lit(111).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    )

    write_df_in_assets(insert_ast_to_ast_df, "dem", "asset_relationship")

    # insert_ast_cols_df is used to insert new column as an asset in ast column table
    insert_cols_details = insert_cols_details.withColumn('COL_ID', F.expr('uuid()'))
    insert_ast_cols_df = insert_cols_details.select(
        'COL_ID',
        F.col('cols_ast_id').alias("AST_ID"),
        F.col("tbl_id").alias("TBL_ID"),
        F.col("schm_id").alias("SCHM_ID"),
        F.col("columnName").alias("COL_NM"),
        F.col("comment").alias("COL_DE"),
        F.col("data_type").alias("DATA_TYPE"),
        F.lit("active").alias("STATUS"),
        F.lit(0).alias("ACTIVE_FL"),
        F.lit(111).alias("creat_btch_id"),
        F.lit("zkgu3t8").alias("creat_nm"),
        F.current_timestamp().alias("creat_ts")
    )

    write_df_in_assets(insert_ast_cols_df, "dem", "asset_column")

    schm_tbl_df.unpersist()
    ast_updt_col_df.unpersist()
    insert_cols_details.unpersist()

    # delete_df = get_cols_asset_ids_to_be_deleted(update_cols_df, ast_cols_hierarchy_df)

def update_df_records(update_tables_df, block_to_read, updt_tbl_query):
    if update_tables_df.count() > 0:
        update_tables_df = update_tables_df.toDF(*[col.lower() for col in update_tables_df.columns])

        update_config = load_properties("dcm")
        update_query_template = update_config[block_to_read][updt_tbl_query]

        column_mapping = {key: value for key, value in update_config[block_to_read].items() if key != updt_tbl_query}

        update_tables_df.foreachPartition(
            lambda iterator: sparkUtil.update_records(iterator, update_config, update_query_template, column_mapping)
        )

def get_table_details(json_df):
    return (json_df
            .withColumn("databaseName", F.col("tableInfo.Database:"))
            .withColumn("cluster_info", F.col("tableInfo.cluster_info"))
            .withColumn("OwnerType", F.col("tableInfo.OwnerType:"))
            .withColumn("Owner", F.col("tableInfo.Owner:"))
            .withColumn("CreateTime", F.col("tableInfo.CreateTime:"))
            .withColumn("LastAccessTime", F.col("tableInfo.LastAccessTime:"))
            .withColumn("Retention", F.col("tableInfo.Retention:"))
            .withColumn("TableType", F.col("tableInfo.Table Type:"))
            .withColumn("TableParameters", F.col("tableInfo.Table Parameters:"))
            .withColumn("SerDeLib", F.col("storageInformation.SerDe Library:"))
            .withColumn("InputFormat", F.col("storageInformation.InputFormat:"))
            .withColumn("OutputFormat", F.col("storageInformation.OutputFormat:"))
            .withColumn("Compressed", F.when(F.col("storageInformation.Compressed:") == F.lit("No"), 'N').otherwise('Y'))
            .withColumn("NumBuckets", F.col("storageInformation.Num Buckets:"))
            .withColumn("BucketColumns", F.col("storageInformation.Bucket Columns:"))
            .withColumn("SortColumns", F.col("storageInformation.Sort Columns:"))
            .withColumn("vw_def", F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("EXTERNAL"), None)
                        .otherwise(F.col("viewInformation.View Expanded Text:")))
            .withColumn("location", F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("VIRTUAL_VIEW"), None)
                        .otherwise(F.col("storageInformation.location")))
            .withColumn("ait_name", F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("VIRTUAL_VIEW"),
                                           F.regexp_replace(F.split(F.col("tableInfo.Database:"), '_')[0], '^', ''))
                        .otherwise(F.split(F.col("storageInformation.location"), '/')[4]))
            .select("OwnerType", "Owner", "CreateTime", "LastAccessTime", "Retention", "TableType", "TableParameters",
                    "tableName", "databaseName", "cluster_info", "SerDeLib", "InputFormat", "OutputFormat", "Compressed",
                    "NumBuckets", "BucketColumns", "SortColumns", "location", "vw_def", "ait_name"))


def get_column_details(json_df):
    asset_columns_df = (json_df
                        .select(F.explode(F.col("columns")).alias("columns"),
                                "tableName",
                                F.col("tableInfo.Database:").alias("databaseName")
                                #, F.col("tableInfo.Table Type:").alias("TableType")
                                , F.when(F.upper(F.col("tableInfo.Table Type:")) == F.lit("VIRTUAL_VIEW")
                                         #, F.regexp_replace(F.split(F.col("tableInfo.Database:"), '')[0], '^', '')
                                         #.otherwise(F.split(F.col("storageInformation.location"), '7')[4]).alias("ait_name")
                                         )
                                )
    # return asset_columns_df.select("columns.*", "tableName", "databaseName", "ait_name", "TableType") .*", "tableName", "databaseName")
    return asset_columns_df.select("columns.*", "tableName", "databaseName")

def get_updt_insert_tables_df(table_details_df, ast_hierarchy_df):
    join_df = table_details_df.join(ast_hierarchy_df,
                                    (F.upper(table_details_df['databaseName']) == F.upper(ast_hierarchy_df['database_name'])) &
                                    (F.upper(table_details_df['tableName']) == F.upper(ast_hierarchy_df['table_name'])),
                                    'left')
    update_tables_df, insert_tables_df = get_updt_insert_df(join_df)
    return update_tables_df, insert_tables_df

def get_updt_insert_cols_df(col_details_df, ast_hierarchy_df):
    join_df = col_details_df.join(ast_hierarchy_df,
                                    (F.upper(col_details_df['databaseName']) == F.upper(ast_hierarchy_df['database_name'])) &
                                    (F.upper(col_details_df['tableName']) == F.upper(ast_hierarchy_df['table_name'])) &
                                    (F.upper(col_details_df['columnName']) == F.upper(ast_hierarchy_df['column_name'])),
                                    'left')
    update_cols_df, insert_cols_df = get_updt_insert_df(join_df)
    return update_cols_df, insert_cols_df


def get_updt_insert_df(join_df):
    update_df = join_df.filter(join_df.AST_NM.isNotNull())
    # update_df.show()
    
    drop_nulls_cols_df = join_df.subtract(update_df)
    
    insert_df = drop_nulls_cols_df.drop(F.col("AST_ID"), F.col("AST_NM"), F.col("AST_TYP_CD"), F.col("AST_TYP_ID"), F.col("SOURCE_NAME_LVL1"), F.col("SOURCE_ID_LVL1"), F.col("SOURCE_NAME_LVL2"), F.col("SOURCE_ID_LVL2"), F.col("database_name"), F.col("table_name"), F.col("column_name")).withColumn('ast_id', F.expr('uuid()'))
    # insert_df.show()
    
    return update_df, insert_df


def write_df_in_assets(df_to_write, properties_file_name, table_name):
    if df_to_write.count() > 0:
        print("=======going to insert data in " + table_name + "========")
        # sparkUtil.write_dataframe_to_db(df_to_write, properties_file_name, table_name)

please provide document analysis for above code

ChatGPT
The provided code appears to be a Python script that utilizes Apache Spark for processing and analyzing data from a JSON file. Let's break down the main components and functionalities of the code:

Import Statements:

python
Copy code
from dem.utilities import sparkDBUtility as sparkUtil
from dem.utilities import helperUtils as helper
import pyspark.sql.functions as F
from dem.properties import load_properties
The script imports utility modules for Spark database operations, helper functions, PySpark SQL functions, and a properties loading module.
Function harvest_atlas(spark):

This function is the main entry point for data processing.
It loads properties from a JSON file using load_properties.
Reads a JSON file using sparkUtil.create_json_dataframe.
Expands the "files" column using PySpark's explode and selects the relevant columns.
Calls various functions (get_table_details, get_column_details, captured_ait_details, cluster_level_exec, schema_level_exec, tables_level_exec, columns_level_exec) to process and analyze the data.
These functions seem to involve processing hierarchical data, handling clusters, schemas, tables, and columns, and updating the corresponding database.
Functions cluster_level_exec, schema_level_exec, tables_level_exec, columns_level_exec:

These functions appear to handle specific levels of hierarchy (cluster, schema, tables, and columns) within the data.
They involve joining, updating, and inserting data into various tables based on the hierarchical relationships.
Functions get_schema_and_tables_ids, captured_ait_details, get_updt_insert_df, get_table_details, get_column_details:

These functions perform specific tasks such as getting schema and table IDs, capturing AIT (Application Integration Table) details, extracting table and column details, and getting update/insert dataframes.
Functions update_df_records, write_df_in_assets:

These functions seem to handle updating and writing dataframes into a database.
Miscellaneous:

The code contains some debug prints, which can be helpful for understanding the flow of execution.
It seems to be designed to work with a Spark environment for distributed data processing.
Note:

The code includes some commented-out sections (# json_df.show(), # col_details_df = captured_ait_details(spark, json_col_details_df, appln_df).drop('ait_name'), etc.) that might be for debugging purposes.
Overall, the code is complex and tailored for a specific data processing pipeline. Understanding the specific data model, database schema, and the purpose of the processing would be necessary for a more detailed analysis.



User
Please provide brief analysis 

ChatGPT
Certainly! Here's a brief analysis of the provided code:

Import Statements: The code begins with import statements, including modules from PySpark and custom utility functions from the dem package.

harvest_atlas Function: The main function orchestrates the extraction of information from a JSON file and the subsequent processing of hierarchical data, such as clusters, schemas, tables, and columns. It utilizes various utility functions for these tasks.

Data Processing Functions:

cluster_level_exec: Deals with processing cluster-level information and updating/inserting relevant details into the database.
schema_level_exec: Handles schema-level processing, updating/inserting schema details.
tables_level_exec: Manages table-level processing, updating/inserting table details.
columns_level_exec: Manages column-level processing, updating/inserting column details.
Utility Functions:

Functions like get_updt_insert_df, get_schema_and_tables_ids, update_df_records, and others are utility functions used for data manipulation and update/insert operations.
DataFrame Manipulation:

The code extensively uses PySpark DataFrames for data manipulation, filtering, and joining.
Database Interaction:

There are calls to functions such as sparkUtil.create_db_dataframe and sparkUtil.write_dataframe_to_db, suggesting interaction with a database.
Data Extraction and Transformation:

Extracting information from JSON, exploding columns, and creating new DataFrames for subsequent processing.
Error Handling:

There is a basic error handling approach using if conditions to check if DataFrames have records before performing certain operations.
Comments:

Comments are used throughout the code to provide explanations for various sections.
Overall Structure:

The code follows a modular structure, with functions dedicated to specific tasks, enhancing readability and maintainability.
Improvement Suggestions:

Some functions are lengthy, and breaking them down into smaller functions might improve code organization.
It would be helpful to have more context on the specific database operations and the purpose of the code for a more detailed analysis.
Remember, this analysis is based on a high-level understanding, and for a more detailed analysis, knowledge of the specific requirements and the external utilities used (sparkUtil) would be necessary.




Message ChatGPT…

ChatGPT can make mistakes. Consider checking important information.
ChatGPT
