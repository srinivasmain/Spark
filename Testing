from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Assuming spark is your SparkSession and df1, df2 are your PySpark DataFrames

# Join dataframes on specified conditions
joined_df = df1.join(df2, 
                     (col("TABLE ASSET NAME") == col("tableName")) & (col("SCHEMA_NAME") == col("databaseName")), 
                     how="outer")

# Identify columns from df1 that are not matched with df2
df1_not_matched = joined_df.filter(col("columnName").isNull()).select(df1.columns)

# Identify columns from df2 that are not matched with df1
df2_not_matched = joined_df.filter(col("TABLE ASSET NAME").isNull()).select(df2.columns)

# Display the results
print("Columns from df1 not matched with df2:")
df1_not_matched.show()

print("\nColumns from df2 not matched with df1:")
df2_not_matched.show()
