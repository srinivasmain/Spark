import unittest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from your_module import get_updt_insert_df  # Make sure to replace 'your_module' with the actual module name

class TestGetUpdtInsertDf(unittest.TestCase):

    def setUp(self):
        self.spark = SparkSession.builder.master("local[2]").appName("test").getOrCreate()

    def tearDown(self):
        self.spark.stop()

    def test_get_updt_insert_df(self):
        # Sample data
        data = [
            (1, "Name1", "Type1", 101, "Source1", 201, "Source2", 301, "db1", "table1", "col1"),
            (2, "Name2", "Type2", 102, "Source3", 202, "Source4", 302, "db2", "table2", "col2"),
            (3, None, "Type3", 103, "Source5", 203, "Source6", 303, "db3", "table3", "col3"),
            # Add more sample data as needed
        ]

        schema = [
            "AST_ID", "AST_NM", "AST_TYP_CD", "AST_TYP_ID",
            "SOURCE_NAME_LVL1", "SOURCE_ID_LVL1", "SOURCE_NAME_LVL2", "SOURCE_ID_LVL2",
            "database_name", "table_name", "column_name"
        ]

        # Creating a DataFrame
        df = self.spark.createDataFrame(data, schema=schema)

        # Calling the function
        update_df, insert_df = get_updt_insert_df(df)

        # Assertions
        self.assertEqual(update_df.count(), 2)  # Replace with the expected count
        self.assertEqual(insert_df.count(), 1)  # Replace with the expected count

        # Additional assertions as needed
        # ...


if __name__ == '__main__':
    unittest.main()
