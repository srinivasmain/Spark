"""
Script for Hive Technical Metadata Harvesting using PySpark.

Usage:
- Ensure you have the necessary modules: `sparkUtil` and `hiveTechMD`.
- Modify the script according to your specific requirements.

Script Flow:
1. Initializes variables for start_time, end_time, and json_stats.
2. Creates a Spark session named "spark" using the sparkUtil module.
3. Records the start time of the job.
4. Specifies the harvesting system as "Hive_Technical_Metadata_Harvesting".
5. Creates a database dataframe for the "dcm" database and "job_status_tab" table.
6. If the harvesting system is "Hive_Technical_Metadata_Harvesting":
    a. Defines a parser with a sample SQL query.
    b. Calls the hiveTechMD module's method to harvest technical metadata using Spark.
    # c. Potentially calls the atlas module to harvest metadata from Atlas (commented out).
7. Stops the Spark session.

Note:
- Make sure to replace or modify specific parts of the script based on your actual requirements.
"""

from datetime import datetime
import sparkUtil  # Assuming you have a module named sparkUtil
import hiveTechMD  # Assuming you have a module named hiveTechMD

if __name__ == "__main__":
    start_time = None
    end_time = None
    json_stats = None

    # Step 2: Create a Spark session
    spark = sparkUtil.create_spark_session("spark")

    # Step 3: Record job start time
    job_start_time = datetime.now()

    # Step 4: Specify harvesting system
    harvesting_system = "Hive_Technical_Metadata_Harvesting"

    # Step 5: Create a database dataframe
    sparkUtil.create_db_dataframe(spark, "dcm", "job_status_tab")

    # Step 6: Perform actions based on the harvesting system
    if harvesting_system == "Hive_Technical_Metadata_Harvesting":
        # Step 6a: Define a parser
        parser = Parser("SELECT VPRC PROD_ENGN_PRCS_FC.sessn_id AS SESSN_ID")

        # Step 6b: Call the method to harvest technical metadata
        hiveTechMD.harvest_technical_metadata(spark)

        # Step 6c: Potentially call the atlas module (commented out)
        # start_time, end time, json_stats = atlas.harvest_atlas(spark)

    # Step 7: Stop the Spark session
    spark.stop()
